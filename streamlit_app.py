import streamlit as st
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import json
import os
from pathlib import Path
from collections import deque
import warnings
warnings.filterwarnings('ignore')

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="OHT-Fire AI Agent",
    page_icon="ğŸ”¥",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ============================================
# Session State ì´ˆê¸°í™”
# ============================================
if 'prediction_history' not in st.session_state:
    st.session_state.prediction_history = {}  # {csv_file: deque of results}

if 'current_model' not in st.session_state:
    st.session_state.current_model = None

# ============================================
# ë°ì´í„° ìŠ¤íƒ ê´€ë¦¬ í•¨ìˆ˜
# ============================================
def update_prediction_stack(model_name, csv_files, max_history=1000):
    """
    CSV íŒŒì¼ ê¸°ë°˜ìœ¼ë¡œ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ìŠ¤íƒì— ì €ì¥
    - CSV íŒŒì¼ì´ ë³€ê²½ë˜ë©´ ìƒˆë¡œ ì‹œì‘
    - ê°™ì€ CSV íŒŒì¼ì´ë©´ ê²°ê³¼ë¥¼ ëˆ„ì 
    """
    current_csv_key = tuple(sorted(csv_files))
    
    # ëª¨ë¸ì´ ë³€ê²½ë˜ì—ˆê±°ë‚˜ CSV íŒŒì¼ì´ ë³€ê²½ëœ ê²½ìš° ì´ˆê¸°í™”
    if st.session_state.current_model != model_name or current_csv_key not in st.session_state.prediction_history:
        st.session_state.current_model = model_name
        st.session_state.prediction_history[current_csv_key] = {
            'data': deque(maxlen=max_history),
            'csv_files': csv_files,
            'model': model_name
        }
    
    return st.session_state.prediction_history[current_csv_key]

def add_prediction_to_stack(stack, prediction_data):
    """ì˜ˆì¸¡ ë°ì´í„°ë¥¼ ìŠ¤íƒì— ì¶”ê°€"""
    stack['data'].append(prediction_data)

def get_stacked_predictions(stack):
    """ìŠ¤íƒëœ ëª¨ë“  ì˜ˆì¸¡ ë°ì´í„° ë°˜í™˜"""
    if stack['data']:
        # ìµœì‹  ë°ì´í„° ìˆœì„œëŒ€ë¡œ ë°˜í™˜
        return list(stack['data'])
    return []

def _display_sample_explanation(explanation, sample_idx):
    """
    ìƒ˜í”Œë³„ ì„¤ëª… í‘œì‹œ (Instance-level SHAP explanation)
    """
    # ê¸°ë³¸ ì •ë³´
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        pred_class = explanation['predicted_class']
        pred_prob = explanation['predicted_probability']
        st.metric(
            "Predicted Class",
            f"Class {pred_class}",
            f"{pred_prob:.1%}"
        )
    
    with col2:
        true_class = explanation['true_class']
        is_correct = explanation['is_correct']
        status = "âœ… Correct" if is_correct else "âŒ Wrong"
        st.metric(
            "True Class",
            f"Class {true_class}",
            status
        )
    
    with col3:
        contrast_class = explanation['contrast_class']
        contrast_prob = explanation['contrast_probability']
        st.metric(
            "2nd Highest Class",
            f"Class {contrast_class}",
            f"{contrast_prob:.1%}"
        )
    
    with col4:
        margin = explanation['predicted_probability'] - explanation['contrast_probability']
        st.metric(
            "Confidence Margin",
            f"{margin:.1%}",
            delta=None
        )
    
    st.markdown("---")
    
    # í™•ë¥  ë¶„í¬
    st.subheader("Class Probability Distribution")
    probs_dict = explanation['all_class_probabilities']
    class_labels = sorted(probs_dict.keys())
    class_probs = [probs_dict[label] for label in class_labels]
    
    fig, ax = plt.subplots(figsize=(10, 4))
    colors = ['#ff6b6b' if label == f'class_{explanation["predicted_class"]}' else '#4dabf7' 
              for label in class_labels]
    bars = ax.bar(class_labels, class_probs, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)
    ax.set_ylabel('Probability', fontsize=12, fontweight='bold')
    ax.set_title(f'Predicted Class Probabilities - Sample {sample_idx}', fontsize=13, fontweight='bold')
    ax.set_ylim([0, 1])
    ax.grid(True, alpha=0.3, axis='y')
    
    # í™•ë¥  ê°’ í‘œì‹œ
    for bar, prob in zip(bars, class_probs):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
               f'{prob:.2%}', ha='center', va='bottom', fontweight='bold', fontsize=10)
    
    plt.tight_layout()
    st.pyplot(fig)
    
    st.markdown("---")
    
    # ì˜ˆì¸¡ì— ê¸°ì—¬í•œ ìƒìœ„ íŠ¹ì„± (Instance-level)
    st.subheader(f"ğŸ¯ Top Features That Predicted Class {explanation['predicted_class']}")
    
    top_features = explanation['top_contributing_features']
    
    # ìƒìœ„ íŠ¹ì„± ì‹œê°í™”
    fig, ax = plt.subplots(figsize=(10, 5))
    feature_names = [f"Feature {f['feature_idx']}" for f in top_features]
    contributions = [f['contribution_magnitude'] for f in top_features]
    colors_features = plt.cm.RdYlGn_r(np.linspace(0.3, 0.7, len(feature_names)))
    
    bars = ax.barh(feature_names, contributions, color=colors_features, edgecolor='black', linewidth=1.5)
    ax.set_xlabel('Contribution Magnitude |SHAP|', fontsize=11, fontweight='bold')
    ax.set_title(f'Top Contributing Features for Class {explanation["predicted_class"]} - Sample {sample_idx}', 
                fontsize=12, fontweight='bold')
    ax.invert_yaxis()
    
    # ê°’ í‘œì‹œ
    for bar, contrib in zip(bars, contributions):
        width = bar.get_width()
        ax.text(width, bar.get_y() + bar.get_height()/2.,
               f'{contrib:.4f}', ha='left', va='center', fontweight='bold', fontsize=10)
    
    plt.tight_layout()
    st.pyplot(fig)
    
    # íŠ¹ì„±ë³„ ê¸°ì—¬ë„ í…Œì´ë¸”
    st.markdown("#### Feature Contribution Details")
    feature_contribution_df = pd.DataFrame({
        'Rank': range(1, len(top_features) + 1),
        'Feature Index': [f['feature_idx'] for f in top_features],
        'Contribution Score': [f['contribution_magnitude'] for f in top_features],
        'Relative Importance': [f"{f['contribution_magnitude']/contributions[0]*100:.1f}%" for f in top_features]
    })
    st.dataframe(feature_contribution_df, use_container_width=True, hide_index=True)
    
    # ëª¨ë“  íŠ¹ì„±ì˜ ê¸°ì—¬ë„ (íˆíŠ¸ë§µ)
    st.markdown("#### All Features Contribution Heatmap")
    all_contributions = explanation['feature_contribution_scores']
    
    # reshape for visualization (seq_len x num_features í˜•íƒœ ê°€ì •)
    # ë§Œì•½ ë‹¨ìˆœ 1Dë¼ë©´ ê·¸ëŒ€ë¡œ í‘œì‹œ
    fig, ax = plt.subplots(figsize=(14, 3))
    im = ax.imshow([all_contributions], cmap='RdYlGn_r', aspect='auto')
    ax.set_xlabel('Feature Index', fontsize=11, fontweight='bold')
    ax.set_ylabel('Sample', fontsize=11, fontweight='bold')
    ax.set_title(f'All Features Contribution Scores for Class {explanation["predicted_class"]}', 
                fontsize=12, fontweight='bold')
    ax.set_yticks([0])
    ax.set_yticklabels([f'Sample {sample_idx}'])
    
    plt.colorbar(im, ax=ax, label='|SHAP value|')
    plt.tight_layout()
    st.pyplot(fig)
    
    # í•´ì„
    st.markdown("#### ğŸ“‹ Interpretation")
    explanation_text = f"""
    **Summary:** This sample was predicted as **Class {explanation['predicted_class']}** 
    with a confidence of **{explanation['predicted_probability']:.1%}**.
    
    **Top Contributing Features:** The model's decision was primarily influenced by:
    """
    for i, feature in enumerate(top_features[:3], 1):
        explanation_text += f"\n{i}. Feature {feature['feature_idx']} (score: {feature['contribution_magnitude']:.4f})"
    
    explanation_text += f"""
    
    **Prediction Confidence:** The margin between the predicted class and the second-highest class 
    is {explanation['predicted_probability'] - explanation['contrast_probability']:.1%}, 
    indicating a {'strong' if explanation['predicted_probability'] - explanation['contrast_probability'] > 0.3 else 'moderate' if explanation['predicted_probability'] - explanation['contrast_probability'] > 0.15 else 'weak'} prediction.
    
    **Correctness:** This prediction is {'âœ… CORRECT' if explanation['is_correct'] else 'âŒ INCORRECT'}.
    """
    
    st.info(explanation_text)

# ì»¤ìŠ¤í…€ CSS
st.markdown("""
<style>
    .main {
        padding-top: 2rem;
    }
    .metric-card {
        background-color: #f0f2f6;
        padding: 1.5rem;
        border-radius: 0.5rem;
        margin: 0.5rem;
    }
    .alert-high {
        background-color: #ffcccc;
        padding: 1rem;
        border-radius: 0.5rem;
        border-left: 4px solid #ff0000;
    }
    .alert-medium {
        background-color: #fff3cd;
        padding: 1rem;
        border-radius: 0.5rem;
        border-left: 4px solid #ffc107;
    }
    .alert-low {
        background-color: #d4edda;
        padding: 1rem;
        border-radius: 0.5rem;
        border-left: 4px solid #28a745;
    }
</style>
""", unsafe_allow_html=True)

# ì œëª©
st.title("ğŸ”¥ OHT Fire - AI Prediction & XAI Dashboard with Data Stacking")
st.markdown("---")

# ì‚¬ì´ë“œë°” ì„¤ì •
st.sidebar.title("âš™ï¸ Settings")

# 1. ëª¨ë¸ ì„ íƒ
available_models = []
results_dir = Path('./test_results')
if results_dir.exists():
    available_models = [d.name for d in results_dir.iterdir() if d.is_dir()]

selected_model = st.sidebar.selectbox(
    "Select Model",
    available_models,
    help="Choose a trained model to analyze"
)

# 2. XAI ê²°ê³¼ ë¡œë“œ
xai_dir = Path('./xai_results') / selected_model
xai_available = xai_dir.exists()

st.sidebar.info(
    f"âœ“ XAI Results Available" if xai_available else "âœ— XAI Results Not Available"
)

# ë°ì´í„° ìŠ¤íƒ ì„¤ì •
st.sidebar.markdown("---")
st.sidebar.subheader("ğŸ“Š Data Stacking Settings")

max_history = st.sidebar.slider(
    "Maximum Stacked Records",
    min_value=100,
    max_value=5000,
    value=1000,
    step=100,
    help="Maximum number of predictions to keep in history"
)

if st.sidebar.button("ğŸ”„ Clear History", help="Clear all stacked prediction data"):
    st.session_state.prediction_history = {}
    st.session_state.current_model = None
    st.success("History cleared!")

# ë©”ì¸ ì»¨í…ì¸ 
if selected_model:
    
    # Tab ìƒì„±
    tab1, tab2, tab2_5, tab3, tab4, tab5 = st.tabs([
        "ğŸ“Š Model Predictions",
        "ğŸ” XAI Analysis",
        "ğŸ¯ Sample Explanation",
        "ğŸ“ˆ Feature Importance",
        "âš ï¸ Alarm & Insights",
        "ğŸ“š Prediction History"
    ])
    
    # ============================================
    # TAB 1: Model Predictions
    # ============================================
    with tab1:
        st.subheader("Model Prediction Results")
        
        # ì˜ˆì¸¡ ê²°ê³¼ íŒŒì¼ ë¡œë“œ
        analysis_file = xai_dir / 'xai_analysis.json' if xai_available else None
        
        if analysis_file and analysis_file.exists():
            with open(analysis_file, 'r') as f:
                analysis_data = json.load(f)
            
            # ë°ì´í„° ìŠ¤íƒ ì—…ë°ì´íŠ¸
            csv_files = ["test_data"]  # ì‹¤ì œë¡œëŠ” file_mappingì—ì„œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŒ
            stack = update_prediction_stack(selected_model, csv_files, max_history)
            add_prediction_to_stack(stack, analysis_data)
            
            # ë©”íŠ¸ë¦­ í‘œì‹œ
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric(
                    "Total Samples",
                    analysis_data['total_samples'],
                    help="Number of test samples analyzed"
                )
            
            with col2:
                st.metric(
                    "Number of Classes",
                    analysis_data['num_classes'],
                    help="Number of fire classes"
                )
            
            with col3:
                accuracy = analysis_data['model_accuracy']
                st.metric(
                    "Model Accuracy",
                    f"{accuracy:.2%}",
                    help="Accuracy on test dataset"
                )
            
            with col4:
                stacked_count = len(list(stack['data']))
                st.metric(
                    "Stacked Records",
                    stacked_count,
                    help=f"Number of stacked predictions (max: {max_history})"
                )
            
            # ì˜ˆì¸¡ ê²°ê³¼ ë¶„í¬
            st.subheader("Prediction Distribution")
            
            predictions = np.array(analysis_data['predictions'])
            true_labels = np.array(analysis_data['true_labels'])
            
            col1, col2 = st.columns(2)
            
            with col1:
                # ì˜ˆì¸¡ í´ë˜ìŠ¤ ë¶„í¬
                pred_dist = pd.Series(predictions).value_counts().sort_index()
                fig, ax = plt.subplots(figsize=(8, 5))
                colors = plt.cm.Set3(range(len(pred_dist)))
                bars = ax.bar(
                    [f'Class {i}' for i in pred_dist.index],
                    pred_dist.values,
                    color=colors,
                    edgecolor='black',
                    linewidth=1.5
                )
                ax.set_ylabel('Number of Samples', fontsize=11, fontweight='bold')
                ax.set_title('Predicted Class Distribution', fontsize=12, fontweight='bold')
                ax.grid(axis='y', alpha=0.3)
                
                # ê°’ ë¼ë²¨
                for bar in bars:
                    height = bar.get_height()
                    ax.text(bar.get_x() + bar.get_width()/2., height,
                           f'{int(height)}', ha='center', va='bottom', fontweight='bold')
                
                plt.tight_layout()
                st.pyplot(fig)
            
            with col2:
                # ì‹¤ì œ í´ë˜ìŠ¤ ë¶„í¬
                true_dist = pd.Series(true_labels).value_counts().sort_index()
                fig, ax = plt.subplots(figsize=(8, 5))
                colors = plt.cm.Set3(range(len(true_dist)))
                bars = ax.bar(
                    [f'Class {i}' for i in true_dist.index],
                    true_dist.values,
                    color=colors,
                    edgecolor='black',
                    linewidth=1.5
                )
                ax.set_ylabel('Number of Samples', fontsize=11, fontweight='bold')
                ax.set_title('True Class Distribution', fontsize=12, fontweight='bold')
                ax.grid(axis='y', alpha=0.3)
                
                # ê°’ ë¼ë²¨
                for bar in bars:
                    height = bar.get_height()
                    ax.text(bar.get_x() + bar.get_width()/2., height,
                           f'{int(height)}', ha='center', va='bottom', fontweight='bold')
                
                plt.tight_layout()
                st.pyplot(fig)
            
            st.markdown("---")
            
            # Confidence Distribution
            st.subheader("Prediction Confidence Distribution")
            
            probs = np.array(analysis_data['prediction_probabilities'])
            max_probs = np.max(probs, axis=1)
            
            fig, ax = plt.subplots(figsize=(10, 5))
            ax.hist(max_probs, bins=30, color='skyblue', edgecolor='black', alpha=0.7)
            ax.axvline(np.mean(max_probs), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(max_probs):.3f}')
            ax.set_xlabel('Confidence Score', fontsize=11, fontweight='bold')
            ax.set_ylabel('Frequency', fontsize=11, fontweight='bold')
            ax.set_title('Model Prediction Confidence Distribution', fontsize=12, fontweight='bold')
            ax.legend()
            ax.grid(axis='y', alpha=0.3)
            plt.tight_layout()
            st.pyplot(fig)
            
            # ìƒì„¸ í…Œì´ë¸”
            st.subheader("Detailed Predictions (First 50 samples)")
            
            results_df = pd.DataFrame({
                'Sample ID': range(len(predictions[:50])),
                'Predicted Class': predictions[:50],
                'True Class': true_labels[:50],
                'Correct': (predictions[:50] == true_labels[:50]).astype(int),
                'Confidence': np.max(probs[:50], axis=1),
            })
            
            st.dataframe(results_df, use_container_width=True)
        
        else:
            st.warning("No prediction results found. Please run the model first.")
    
    # ============================================
    # TAB 2: XAI Analysis
    # ============================================
    with tab2:
        st.subheader("SHAP-based Explainability Analysis")
        
        if xai_available:
            # Summary í…ìŠ¤íŠ¸ í‘œì‹œ
            summary_file = xai_dir / 'xai_summary.txt'
            if summary_file.exists():
                with open(summary_file, 'r') as f:
                    summary_text = f.read()
                st.text(summary_text)
            
            st.markdown("---")
            
            # SHAP Values í†µê³„
            st.subheader("SHAP Values Statistics")
            
            # ê° í´ë˜ìŠ¤ì˜ SHAP values ë¡œë“œ
            shap_files = list(xai_dir.glob('shap_values_class_*.npy'))
            
            if shap_files:
                shap_stats = {}
                for shap_file in sorted(shap_files):
                    class_idx = shap_file.stem.split('_')[-1]
                    shap_values = np.load(shap_file)
                    
                    shap_stats[f'Class {class_idx}'] = {
                        'Mean |SHAP|': np.mean(np.abs(shap_values)),
                        'Std |SHAP|': np.std(np.abs(shap_values)),
                        'Max |SHAP|': np.max(np.abs(shap_values)),
                        'Shape': f"{shap_values.shape}",
                    }
                
                stats_df = pd.DataFrame(shap_stats).T
                st.dataframe(stats_df, use_container_width=True)
        else:
            st.warning("XAI analysis results not found. Please run with --use_xai flag.")
    
    # ============================================
    # TAB 2.5: Sample Explanation (Instance-level)
    # ============================================
    with tab2_5:
        st.subheader("ğŸ¯ Why Was This Sample Predicted as This Class?")
        
        if xai_available:
            # sample_explanations.json ë¡œë“œ
            sample_exp_file = xai_dir / 'sample_explanations.json'
            
            if sample_exp_file.exists():
                with open(sample_exp_file, 'r') as f:
                    sample_explanations = json.load(f)
                
                st.markdown("""
                This section explains **why the model made a specific prediction** for each sample.
                Instead of showing global feature importance, it shows the actual features that contributed
                to each sample being classified as a particular class.
                """)
                
                # ìƒ˜í”Œ ì„ íƒ UI
                col1, col2 = st.columns([3, 1])
                
                with col1:
                    sample_selection = st.selectbox(
                        "Select a sample to explain:",
                        range(len(sample_explanations)),
                        format_func=lambda i: f"Sample {i} â†’ Predicted: Class {sample_explanations[i]['predicted_class']} (True: Class {sample_explanations[i]['true_class']})"
                    )
                
                with col2:
                    show_all = st.checkbox("Show all samples", False)
                
                st.markdown("---")
                
                if show_all:
                    # ëª¨ë“  ìƒ˜í”Œ í‘œì‹œ
                    for idx, exp in enumerate(sample_explanations):
                        with st.expander(
                            f"ğŸ“Œ Sample {idx}: Predicted as **Class {exp['predicted_class']}** "
                            f"(Prob: {exp['predicted_probability']:.2%}) | "
                            f"True: Class {exp['true_class']} {'âœ…' if exp['is_correct'] else 'âŒ'}"
                        ):
                            _display_sample_explanation(exp, idx)
                else:
                    # ì„ íƒëœ ìƒ˜í”Œë§Œ í‘œì‹œ
                    exp = sample_explanations[sample_selection]
                    _display_sample_explanation(exp, sample_selection)
                    
            else:
                st.warning("Sample explanations file not found. Please re-run XAI analysis.")
        else:
            st.warning("XAI analysis results not found. Please run with --use_xai flag.")
    
    # ============================================
    # TAB 4: Feature Importance
    # ============================================
    with tab4:
        st.subheader("Feature Importance Analysis")
        
        if xai_available:
            # Feature importance ì´ë¯¸ì§€ í‘œì‹œ
            importance_img = xai_dir / 'feature_importance.png'
            if importance_img.exists():
                st.image(str(importance_img), use_column_width=True, caption="Feature Importance based on SHAP")
            
            st.markdown("---")
            
            # Feature importance ìˆ˜ì¹˜ í‘œì‹œ
            analysis_file = xai_dir / 'xai_analysis.json'
            if analysis_file.exists():
                with open(analysis_file, 'r') as f:
                    analysis_data = json.load(f)
                
                st.subheader("Feature Importance Values")
                
                importance_dict = analysis_data['feature_importance']
                
                # ê° í´ë˜ìŠ¤ë³„ ìƒìœ„ íŠ¹ì„±
                for class_name, importance_values in importance_dict.items():
                    st.subheader(f"ğŸ“Œ {class_name}")
                    
                    # DataFrameìœ¼ë¡œ ë³€í™˜
                    importance_df = pd.DataFrame({
                        'Feature': [f'Feature {i}' for i in range(len(importance_values))],
                        'Importance': importance_values
                    }).sort_values('Importance', ascending=False)
                    
                    # ìƒìœ„ 10ê°œ
                    top_10 = importance_df.head(10)
                    
                    col1, col2 = st.columns([1.5, 1])
                    
                    with col1:
                        fig, ax = plt.subplots(figsize=(8, 5))
                        colors = plt.cm.viridis(np.linspace(0, 1, len(top_10)))
                        bars = ax.barh(top_10['Feature'], top_10['Importance'], color=colors)
                        ax.set_xlabel('Mean |SHAP value|', fontsize=11, fontweight='bold')
                        ax.set_title(f'Top 10 Important Features - {class_name}', fontsize=12, fontweight='bold')
                        ax.invert_yaxis()
                        
                        for bar in bars:
                            width = bar.get_width()
                            ax.text(width, bar.get_y() + bar.get_height()/2.,
                                   f'{width:.6f}', ha='left', va='center', fontsize=9)
                        
                        plt.tight_layout()
                        st.pyplot(fig)
                    
                    with col2:
                        st.dataframe(top_10, use_container_width=True)
                    
                    st.markdown("---")
        else:
            st.warning("XAI analysis results not found.")
    
    # ============================================
    # TAB 5: Alarm & Insights
    # ============================================
    with tab5:
        st.subheader("âš ï¸ Alarm System & Risk Assessment")
        
        if analysis_file and analysis_file.exists():
            with open(analysis_file, 'r') as f:
                analysis_data = json.load(f)
            
            predictions = np.array(analysis_data['predictions'])
            true_labels = np.array(analysis_data['true_labels'])
            probs = np.array(analysis_data['prediction_probabilities'])
            
            # ì•ŒëŒ ê·œì¹™ ì„¤ì •
            st.subheader("Alarm Rules Configuration")
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                confidence_threshold = st.slider(
                    "Confidence Threshold for High Alert",
                    0.0, 1.0, 0.7,
                    help="Alert when confidence is below this threshold"
                )
            
            with col2:
                high_fire_classes = st.multiselect(
                    "High-Risk Fire Classes",
                    range(analysis_data['num_classes']),
                    default=[3],  # ê°€ì¥ ë†’ì€ ë“±ê¸‰
                    help="Classes considered as high-risk fire"
                )
            
            with col3:
                medium_fire_classes = st.multiselect(
                    "Medium-Risk Fire Classes",
                    range(analysis_data['num_classes']),
                    default=[2],
                    help="Classes considered as medium-risk fire"
                )
            
            st.markdown("---")
            
            # ì•ŒëŒ ìƒì„±
            st.subheader("Generated Alarms")
            
            # ì•ŒëŒ ì¹´ìš´íŠ¸
            high_risk_count = 0
            medium_risk_count = 0
            low_confidence_count = 0
            
            alarm_data = []
            
            for idx in range(len(predictions)):
                pred = predictions[idx]
                confidence = np.max(probs[idx])
                
                alarms = []
                risk_level = "ğŸŸ¢ LOW"
                
                # ê³ ìœ„í—˜ ì²´í¬
                if pred in high_fire_classes:
                    alarms.append("ğŸ”´ HIGH RISK CLASS")
                    risk_level = "ğŸ”´ CRITICAL"
                    high_risk_count += 1
                
                # ì¤‘ìœ„í—˜ ì²´í¬
                elif pred in medium_fire_classes:
                    alarms.append("ğŸŸ  MEDIUM RISK CLASS")
                    risk_level = "ğŸŸ  HIGH"
                    medium_risk_count += 1
                
                # ë‚®ì€ ì‹ ë¢°ë„ ì²´í¬
                if confidence < confidence_threshold:
                    alarms.append(f"âš ï¸ LOW CONFIDENCE ({confidence:.2%})")
                    if risk_level == "ğŸŸ¢ LOW":
                        risk_level = "ğŸŸ¡ MEDIUM"
                    low_confidence_count += 1
                
                if alarms:  # ì•ŒëŒì´ ìˆëŠ” ê²½ìš°ë§Œ
                    alarm_data.append({
                        'Sample ID': idx,
                        'Predicted Class': pred,
                        'Confidence': f"{confidence:.2%}",
                        'Alarms': ' | '.join(alarms),
                        'Risk Level': risk_level,
                        'True Class': true_labels[idx],
                        'Correct': 'âœ“' if pred == true_labels[idx] else 'âœ—'
                    })
            
            # ì•ŒëŒ í†µê³„
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric(
                    "ğŸ”´ High Risk Detected",
                    high_risk_count,
                    help="Number of high-risk fire predictions"
                )
            
            with col2:
                st.metric(
                    "ğŸŸ  Medium Risk Detected",
                    medium_risk_count,
                    help="Number of medium-risk fire predictions"
                )
            
            with col3:
                st.metric(
                    "âš ï¸ Low Confidence",
                    low_confidence_count,
                    help="Number of predictions with low confidence"
                )
            
            with col4:
                total_alarms = len(alarm_data)
                st.metric(
                    "ğŸš¨ Total Alarms",
                    total_alarms,
                    help="Total number of triggered alarms"
                )
            
            st.markdown("---")
            
            # ì•ŒëŒ ìƒì„¸ ë³´ê¸°
            if alarm_data:
                st.subheader("Triggered Alarms (Detailed List)")
                
                alarm_df = pd.DataFrame(alarm_data)
                
                # ìœ„í—˜ë„ë³„ ìƒ‰ìƒ í‘œì‹œ
                st.dataframe(
                    alarm_df,
                    use_container_width=True,
                    height=400
                )
                
                st.markdown("---")
                
                # ì•ŒëŒ ë¶„í¬
                st.subheader("Alarm Distribution")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    risk_counts = alarm_df['Risk Level'].value_counts()
                    fig, ax = plt.subplots(figsize=(8, 5))
                    colors_map = {'ğŸ”´ CRITICAL': '#ff0000', 'ğŸŸ  HIGH': '#ff9900', 'ğŸŸ¡ MEDIUM': '#ffff00', 'ğŸŸ¢ LOW': '#00ff00'}
                    colors = [colors_map.get(risk, '#999999') for risk in risk_counts.index]
                    
                    ax.pie(risk_counts.values, labels=risk_counts.index, autopct='%1.1f%%',
                          colors=colors, startangle=90, textprops={'fontsize': 10, 'weight': 'bold'})
                    ax.set_title('Alarm Distribution by Risk Level', fontsize=12, fontweight='bold')
                    plt.tight_layout()
                    st.pyplot(fig)
                
                with col2:
                    # ì•ŒëŒ ìœ í˜• ë¶„ì„
                    all_alarms = []
                    for alarm_list in alarm_df['Alarms']:
                        all_alarms.extend([a.strip() for a in alarm_list.split('|')])
                    
                    alarm_counts = pd.Series(all_alarms).value_counts()
                    fig, ax = plt.subplots(figsize=(8, 5))
                    ax.barh(range(len(alarm_counts)), alarm_counts.values, color='coral', edgecolor='black')
                    ax.set_yticks(range(len(alarm_counts)))
                    ax.set_yticklabels(alarm_counts.index)
                    ax.set_xlabel('Count', fontsize=11, fontweight='bold')
                    ax.set_title('Alarm Type Distribution', fontsize=12, fontweight='bold')
                    ax.invert_yaxis()
                    
                    for i, v in enumerate(alarm_counts.values):
                        ax.text(v, i, f' {int(v)}', va='center', fontweight='bold')
                    
                    plt.tight_layout()
                    st.pyplot(fig)
                
                st.markdown("---")
                
                # ê¶Œì¥ì‚¬í•­
                st.subheader("ğŸ’¡ Recommendations")
                
                recommendations = []
                
                if high_risk_count > 0:
                    recommendations.append(
                        f"ğŸ”´ **URGENT**: {high_risk_count} high-risk fire predictions detected. "
                        f"Immediate action required for fire prevention and response."
                    )
                
                if medium_risk_count > 0:
                    recommendations.append(
                        f"ğŸŸ  **WARNING**: {medium_risk_count} medium-risk fire predictions detected. "
                        f"Enhanced monitoring and precautions recommended."
                    )
                
                if low_confidence_count > 0:
                    recommendations.append(
                        f"âš ï¸ **CAUTION**: {low_confidence_count} predictions have low confidence scores. "
                        f"Manual verification recommended for these cases."
                    )
                
                if not recommendations:
                    recommendations.append("âœ… **GOOD NEWS**: No alarms triggered. System status normal.")
                
                for rec in recommendations:
                    st.info(rec)
            
            else:
                st.success("âœ… No alarms triggered! All predictions are within safe parameters.")
        
        else:
            st.warning("No prediction results found.")
    
    # ============================================
    # TAB 6: Prediction History (ì‹ ê·œ)
    # ============================================
    with tab6:
        st.subheader("ğŸ“š Stacked Prediction History")
        
        # í˜„ì¬ ìŠ¤íƒ ì •ë³´
        csv_key = None
        for key, stack_info in st.session_state.prediction_history.items():
            if stack_info['model'] == selected_model:
                csv_key = key
                break
        
        if csv_key and st.session_state.prediction_history[csv_key]['data']:
            stack = st.session_state.prediction_history[csv_key]
            stacked_data = get_stacked_predictions(stack)
            
            st.info(
                f"ğŸ“Š **Model**: {stack['model']}\n\n"
                f"**CSV Files**: {', '.join(stack['csv_files'])}\n\n"
                f"**Total Stacked Records**: {len(stacked_data)}"
            )
            
            st.markdown("---")
            
            # ì‹œê°„ì— ë”°ë¥¸ ì •í™•ë„ ì¶”ì´
            if len(stacked_data) > 1:
                st.subheader("Accuracy Trend Over Time")
                
                accuracies = [d['model_accuracy'] for d in stacked_data]
                fig, ax = plt.subplots(figsize=(12, 5))
                ax.plot(range(len(accuracies)), accuracies, marker='o', linewidth=2, markersize=8, color='steelblue')
                ax.axhline(np.mean(accuracies), color='red', linestyle='--', linewidth=2, label=f'Average: {np.mean(accuracies):.3f}')
                ax.set_xlabel('Prediction Index', fontsize=11, fontweight='bold')
                ax.set_ylabel('Accuracy', fontsize=11, fontweight='bold')
                ax.set_title('Model Accuracy Over Stacked Predictions', fontsize=12, fontweight='bold')
                ax.set_ylim([0, 1])
                ax.grid(True, alpha=0.3)
                ax.legend()
                plt.tight_layout()
                st.pyplot(fig)
                
                st.markdown("---")
            
            # í´ë˜ìŠ¤ ë¶„í¬ ëˆ„ì 
            st.subheader("Cumulative Class Predictions")
            
            all_predictions = []
            for d in stacked_data:
                all_predictions.extend(d['predictions'])
            
            cumulative_dist = pd.Series(all_predictions).value_counts().sort_index()
            
            col1, col2 = st.columns([1.5, 1])
            
            with col1:
                fig, ax = plt.subplots(figsize=(10, 6))
                colors = plt.cm.Set3(range(len(cumulative_dist)))
                bars = ax.bar(
                    [f'Class {i}' for i in cumulative_dist.index],
                    cumulative_dist.values,
                    color=colors,
                    edgecolor='black',
                    linewidth=1.5
                )
                ax.set_ylabel('Total Predictions', fontsize=11, fontweight='bold')
                ax.set_title('Cumulative Class Distribution', fontsize=12, fontweight='bold')
                ax.grid(axis='y', alpha=0.3)
                
                for bar in bars:
                    height = bar.get_height()
                    ax.text(bar.get_x() + bar.get_width()/2., height,
                           f'{int(height)}', ha='center', va='bottom', fontweight='bold')
                
                plt.tight_layout()
                st.pyplot(fig)
            
            with col2:
                st.dataframe(
                    cumulative_dist.to_frame('Count'),
                    use_container_width=True
                )
            
            st.markdown("---")
            
            # ìƒì„¸ íˆìŠ¤í† ë¦¬ í…Œì´ë¸”
            st.subheader("Detailed History")
            
            history_records = []
            for idx, data in enumerate(stacked_data):
                history_records.append({
                    'Index': idx + 1,
                    'Total Samples': data['total_samples'],
                    'Accuracy': f"{data['model_accuracy']:.2%}",
                    'High Risk': sum(1 for p in data['predictions'] if p == 3),
                    'Medium Risk': sum(1 for p in data['predictions'] if p == 2),
                })
            
            history_df = pd.DataFrame(history_records)
            st.dataframe(history_df, use_container_width=True)
            
            # CSV ë‹¤ìš´ë¡œë“œ
            st.markdown("---")
            csv_export = history_df.to_csv(index=False)
            st.download_button(
                label="ğŸ“¥ Download History as CSV",
                data=csv_export,
                file_name=f"prediction_history_{selected_model}.csv",
                mime="text/csv"
            )
        
        else:
            st.info("â„¹ï¸ No stacked prediction history yet. Run a prediction to start collecting data.")

else:
    st.info("ğŸ‘ˆ Please select a model from the sidebar to get started.")

# Footer
st.markdown("---")
st.markdown(
    """
    <div style='text-align: center'>
        <p style='color: #888'>OHT Fire - AI Prediction & XAI Dashboard with Data Stacking | Powered by Streamlit</p>
        <p style='color: #aaa; font-size: 0.8em'>Â© 2026 | Model: TimesNet | XAI: SHAP | Stacking: Deque</p>
    </div>
    """,
    unsafe_allow_html=True
)
